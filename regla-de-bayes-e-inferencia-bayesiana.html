<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Estadística Computacional</title>
  <meta name="description" content="Curso de estadística computacional, Maestría en Ciencia de Datos, ITAM 2018.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Estadística Computacional" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Curso de estadística computacional, Maestría en Ciencia de Datos, ITAM 2018." />
  <meta name="github-repo" content="tereom/est-computacional-2018" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Estadística Computacional" />
  
  <meta name="twitter:description" content="Curso de estadística computacional, Maestría en Ciencia de Datos, ITAM 2018." />
  

<meta name="author" content="María Teresa Ortiz">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="probabilidad-subjetiva.html">
<link rel="next" href="distribuciones-conjugadas.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.3/htmlwidgets.js"></script>
<script src="libs/d3-3.5.6/d3.min.js"></script>
<link href="libs/profvis-0.3.5/profvis.css" rel="stylesheet" />
<script src="libs/profvis-0.3.5/profvis.js"></script>
<link href="libs/highlight-6.2.0/textmate.css" rel="stylesheet" />
<script src="libs/highlight-6.2.0/highlight.js"></script>
<script src="libs/profvis-binding-0.3.5/profvis.js"></script>
<script src="libs/plotly-binding-4.8.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.39.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.39.2/plotly-latest.min.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />
<link rel="stylesheet" href="css/font-awesome.min.css" type="text/css" />
<link rel="stylesheet" href="css/cajas.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Estadística Computacional</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Información del curso</a><ul>
<li class="chapter" data-level="" data-path="temario.html"><a href="temario.html"><i class="fa fa-check"></i>Temario</a><ul>
<li class="chapter" data-level="" data-path="temario.html"><a href="temario.html#calificacion"><i class="fa fa-check"></i>Calificación</a></li>
<li class="chapter" data-level="" data-path="temario.html"><a href="temario.html#software"><i class="fa fa-check"></i>Software</a></li>
<li class="chapter" data-level="" data-path="temario.html"><a href="temario.html#otros-recursos"><i class="fa fa-check"></i>Otros recursos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduccion-a-visualizacion.html"><a href="introduccion-a-visualizacion.html"><i class="fa fa-check"></i><b>1</b> Introducción a visualización</a><ul>
<li class="chapter" data-level="" data-path="introduccion-a-visualizacion.html"><a href="introduccion-a-visualizacion.html#el-cuarteto-de-ascombe"><i class="fa fa-check"></i>El cuarteto de Ascombe</a></li>
<li class="chapter" data-level="1.1" data-path="introduccion.html"><a href="introduccion.html"><i class="fa fa-check"></i><b>1.1</b> Introducción</a><ul>
<li class="chapter" data-level="" data-path="introduccion.html"><a href="introduccion.html#visualizacion-de-datos-en-la-estadistica"><i class="fa fa-check"></i>Visualización de datos en la estadística</a></li>
<li class="chapter" data-level="" data-path="introduccion.html"><a href="introduccion.html#visualizacion-popular-de-datos"><i class="fa fa-check"></i>Visualización popular de datos</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="teoria-de-visualizacion-de-datos.html"><a href="teoria-de-visualizacion-de-datos.html"><i class="fa fa-check"></i><b>1.2</b> Teoría de visualización de datos</a><ul>
<li class="chapter" data-level="" data-path="teoria-de-visualizacion-de-datos.html"><a href="teoria-de-visualizacion-de-datos.html#principios-generales-del-diseno-analitico"><i class="fa fa-check"></i>Principios generales del diseño analítico</a></li>
<li class="chapter" data-level="" data-path="teoria-de-visualizacion-de-datos.html"><a href="teoria-de-visualizacion-de-datos.html#tecnicas-de-visualizacion"><i class="fa fa-check"></i>Técnicas de visualización</a></li>
<li class="chapter" data-level="" data-path="teoria-de-visualizacion-de-datos.html"><a href="teoria-de-visualizacion-de-datos.html#indicadores-de-calidad-grafica"><i class="fa fa-check"></i>Indicadores de calidad gráfica</a></li>
<li class="chapter" data-level="" data-path="teoria-de-visualizacion-de-datos.html"><a href="teoria-de-visualizacion-de-datos.html#factor-de-engano-chartjunk-y-pies"><i class="fa fa-check"></i>Factor de engaño, chartjunk y pies</a></li>
<li class="chapter" data-level="" data-path="teoria-de-visualizacion-de-datos.html"><a href="teoria-de-visualizacion-de-datos.html#series-de-tiempo-y-promedio-de-45"><i class="fa fa-check"></i>Series de tiempo y promedio de 45</a></li>
<li class="chapter" data-level="" data-path="teoria-de-visualizacion-de-datos.html"><a href="teoria-de-visualizacion-de-datos.html#pequenos-multiplos-y-densidad-grafica"><i class="fa fa-check"></i>Pequeños múltiplos y densidad gráfica</a></li>
<li class="chapter" data-level="" data-path="teoria-de-visualizacion-de-datos.html"><a href="teoria-de-visualizacion-de-datos.html#tinta-de-datos"><i class="fa fa-check"></i>Tinta de datos</a></li>
<li class="chapter" data-level="" data-path="teoria-de-visualizacion-de-datos.html"><a href="teoria-de-visualizacion-de-datos.html#percepcion-de-escala"><i class="fa fa-check"></i>Percepción de escala</a></li>
<li class="chapter" data-level="" data-path="teoria-de-visualizacion-de-datos.html"><a href="teoria-de-visualizacion-de-datos.html#minard"><i class="fa fa-check"></i>Minard</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduccion-a-r-y-al-paquete-ggplot2.html"><a href="introduccion-a-r-y-al-paquete-ggplot2.html"><i class="fa fa-check"></i><b>2</b> Introducción a R y al paquete ggplot2</a><ul>
<li class="chapter" data-level="2.1" data-path="r-primeros-pasos.html"><a href="r-primeros-pasos.html"><i class="fa fa-check"></i><b>2.1</b> R: primeros pasos</a><ul>
<li class="chapter" data-level="" data-path="r-primeros-pasos.html"><a href="r-primeros-pasos.html#r-en-analisis-de-datos"><i class="fa fa-check"></i>R en análisis de datos</a></li>
<li class="chapter" data-level="" data-path="r-primeros-pasos.html"><a href="r-primeros-pasos.html#paquetes-y-el-tidyverse"><i class="fa fa-check"></i>Paquetes y el Tidyverse</a></li>
<li class="chapter" data-level="" data-path="r-primeros-pasos.html"><a href="r-primeros-pasos.html#recursos"><i class="fa fa-check"></i>Recursos</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="visualizacion-con-ggplot2.html"><a href="visualizacion-con-ggplot2.html"><i class="fa fa-check"></i><b>2.2</b> Visualización con ggplot2</a><ul>
<li class="chapter" data-level="" data-path="visualizacion-con-ggplot2.html"><a href="visualizacion-con-ggplot2.html#recursos-1"><i class="fa fa-check"></i>Recursos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="manipulacion-y-agrupacion-de-datos.html"><a href="manipulacion-y-agrupacion-de-datos.html"><i class="fa fa-check"></i><b>3</b> Manipulación y agrupación de datos</a><ul>
<li class="chapter" data-level="3.1" data-path="transformacion-de-datos.html"><a href="transformacion-de-datos.html"><i class="fa fa-check"></i><b>3.1</b> Transformación de datos</a><ul>
<li><a href="transformacion-de-datos.html#separa-aplica-combina-split-apply-combine">Separa-aplica-combina (<em>split-apply-combine</em>)</a></li>
<li class="chapter" data-level="" data-path="transformacion-de-datos.html"><a href="transformacion-de-datos.html#ejemplos-y-lectura-de-datos"><i class="fa fa-check"></i>Ejemplos y lectura de datos</a></li>
<li class="chapter" data-level="" data-path="transformacion-de-datos.html"><a href="transformacion-de-datos.html#filtrar"><i class="fa fa-check"></i>Filtrar</a></li>
<li class="chapter" data-level="" data-path="transformacion-de-datos.html"><a href="transformacion-de-datos.html#seleccionar"><i class="fa fa-check"></i>Seleccionar</a></li>
<li class="chapter" data-level="" data-path="transformacion-de-datos.html"><a href="transformacion-de-datos.html#ordenar"><i class="fa fa-check"></i>Ordenar</a></li>
<li class="chapter" data-level="" data-path="transformacion-de-datos.html"><a href="transformacion-de-datos.html#mutar"><i class="fa fa-check"></i>Mutar</a></li>
<li class="chapter" data-level="" data-path="transformacion-de-datos.html"><a href="transformacion-de-datos.html#summarise-y-resumenes-por-grupo"><i class="fa fa-check"></i>Summarise y resúmenes por grupo</a></li>
<li class="chapter" data-level="" data-path="transformacion-de-datos.html"><a href="transformacion-de-datos.html#operador-pipeline"><i class="fa fa-check"></i>Operador pipeline</a></li>
<li class="chapter" data-level="" data-path="transformacion-de-datos.html"><a href="transformacion-de-datos.html#variables-por-grupo"><i class="fa fa-check"></i>Variables por grupo</a></li>
<li class="chapter" data-level="" data-path="transformacion-de-datos.html"><a href="transformacion-de-datos.html#verbos-de-dos-tablas"><i class="fa fa-check"></i>Verbos de dos tablas</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="datos-limpios.html"><a href="datos-limpios.html"><i class="fa fa-check"></i><b>3.2</b> Datos limpios</a><ul>
<li class="chapter" data-level="" data-path="datos-limpios.html"><a href="datos-limpios.html#limpieza-bases-de-datos"><i class="fa fa-check"></i>Limpieza bases de datos</a></li>
<li class="chapter" data-level="" data-path="datos-limpios.html"><a href="datos-limpios.html#los-encabezados-de-las-columanas-son-valores"><i class="fa fa-check"></i>Los encabezados de las columanas son valores</a></li>
<li class="chapter" data-level="" data-path="datos-limpios.html"><a href="datos-limpios.html#una-columna-asociada-a-mas-de-una-variable"><i class="fa fa-check"></i>Una columna asociada a más de una variable</a></li>
<li class="chapter" data-level="" data-path="datos-limpios.html"><a href="datos-limpios.html#variables-almacenadas-en-filas-y-columnas"><i class="fa fa-check"></i>Variables almacenadas en filas y columnas</a></li>
<li class="chapter" data-level="" data-path="datos-limpios.html"><a href="datos-limpios.html#mas-de-un-tipo-de-observacion-en-una-misma-tabla"><i class="fa fa-check"></i>Mas de un tipo de observación en una misma tabla</a></li>
<li class="chapter" data-level="" data-path="datos-limpios.html"><a href="datos-limpios.html#una-misma-unidad-observacional-esta-almacenada-en-multiples-tablas"><i class="fa fa-check"></i>Una misma unidad observacional está almacenada en múltiples tablas</a></li>
<li class="chapter" data-level="" data-path="datos-limpios.html"><a href="datos-limpios.html#otras-consideraciones"><i class="fa fa-check"></i>Otras consideraciones</a></li>
<li class="chapter" data-level="" data-path="datos-limpios.html"><a href="datos-limpios.html#recursos-adicionales"><i class="fa fa-check"></i>Recursos adicionales</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="temas-selectos-de-r.html"><a href="temas-selectos-de-r.html"><i class="fa fa-check"></i><b>4</b> Temas selectos de R</a><ul>
<li class="chapter" data-level="4.1" data-path="funciones.html"><a href="funciones.html"><i class="fa fa-check"></i><b>4.1</b> Funciones</a><ul>
<li class="chapter" data-level="" data-path="funciones.html"><a href="funciones.html#estructura-de-una-funcion"><i class="fa fa-check"></i>Estructura de una función</a></li>
<li class="chapter" data-level="" data-path="funciones.html"><a href="funciones.html#observaciones-del-uso-de-funciones"><i class="fa fa-check"></i>Observaciones del uso de funciones</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="vectores.html"><a href="vectores.html"><i class="fa fa-check"></i><b>4.2</b> Vectores</a><ul>
<li class="chapter" data-level="" data-path="vectores.html"><a href="vectores.html#propiedades"><i class="fa fa-check"></i>Propiedades</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="iteracion.html"><a href="iteracion.html"><i class="fa fa-check"></i><b>4.3</b> Iteración</a></li>
<li class="chapter" data-level="4.4" data-path="rendimiento-en-r.html"><a href="rendimiento-en-r.html"><i class="fa fa-check"></i><b>4.4</b> Rendimiento en R</a><ul>
<li class="chapter" data-level="" data-path="rendimiento-en-r.html"><a href="rendimiento-en-r.html#diagnosticar"><i class="fa fa-check"></i>Diagnosticar</a></li>
<li class="chapter" data-level="" data-path="rendimiento-en-r.html"><a href="rendimiento-en-r.html#estrategias-para-mejorar-desempeno"><i class="fa fa-check"></i>Estrategias para mejorar desempeño</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="introduccion-a-probabilidad.html"><a href="introduccion-a-probabilidad.html"><i class="fa fa-check"></i><b>5</b> Introducción a probabilidad</a><ul>
<li class="chapter" data-level="5.1" data-path="probabilidad-como-extension-a-proporcion.html"><a href="probabilidad-como-extension-a-proporcion.html"><i class="fa fa-check"></i><b>5.1</b> Probabilidad como extensión a proporción</a></li>
<li class="chapter" data-level="5.2" data-path="interpretacion-frecuentista-de-probabilidad.html"><a href="interpretacion-frecuentista-de-probabilidad.html"><i class="fa fa-check"></i><b>5.2</b> Interpretación frecuentista de probabilidad</a></li>
<li class="chapter" data-level="5.3" data-path="simulacion-para-el-calculo-de-probabilidades.html"><a href="simulacion-para-el-calculo-de-probabilidades.html"><i class="fa fa-check"></i><b>5.3</b> Simulación para el cálculo de probabilidades</a></li>
<li class="chapter" data-level="5.4" data-path="probabilidad-definicion-matematica.html"><a href="probabilidad-definicion-matematica.html"><i class="fa fa-check"></i><b>5.4</b> Probabilidad: definición matemática</a><ul>
<li class="chapter" data-level="5.4.1" data-path="probabilidad-definicion-matematica.html"><a href="probabilidad-definicion-matematica.html#propiedades-de-la-funcion-de-probabilidad"><i class="fa fa-check"></i><b>5.4.1</b> Propiedades de la función de probabilidad:</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="variables-aleatorias.html"><a href="variables-aleatorias.html"><i class="fa fa-check"></i><b>5.5</b> Variables aleatorias</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bootstrap-no-parametrico.html"><a href="bootstrap-no-parametrico.html"><i class="fa fa-check"></i><b>6</b> Bootstrap no paramétrico</a><ul>
<li class="chapter" data-level="6.1" data-path="el-principio-del-plug-in.html"><a href="el-principio-del-plug-in.html"><i class="fa fa-check"></i><b>6.1</b> El principio del plug-in</a><ul>
<li class="chapter" data-level="" data-path="el-principio-del-plug-in.html"><a href="el-principio-del-plug-in.html#muestras-aleatorias"><i class="fa fa-check"></i>Muestras aleatorias</a></li>
<li class="chapter" data-level="" data-path="el-principio-del-plug-in.html"><a href="el-principio-del-plug-in.html#funcion-de-distribucion-empirica"><i class="fa fa-check"></i>Función de distribución empírica</a></li>
<li class="chapter" data-level="" data-path="el-principio-del-plug-in.html"><a href="el-principio-del-plug-in.html#parametros-y-estadisticas"><i class="fa fa-check"></i>Parámetros y estadísticas</a></li>
<li class="chapter" data-level="" data-path="el-principio-del-plug-in.html"><a href="el-principio-del-plug-in.html#distribuciones-muestrales-y-errores-estandar"><i class="fa fa-check"></i>Distribuciones muestrales y errores estándar</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="el-estimador-bootstrap-del-error-estandar.html"><a href="el-estimador-bootstrap-del-error-estandar.html"><i class="fa fa-check"></i><b>6.2</b> El estimador bootstrap del error estándar</a><ul>
<li class="chapter" data-level="" data-path="el-estimador-bootstrap-del-error-estandar.html"><a href="el-estimador-bootstrap-del-error-estandar.html#variacion-en-distribuciones-bootstrap"><i class="fa fa-check"></i>Variación en distribuciones bootstrap</a></li>
<li class="chapter" data-level="" data-path="el-estimador-bootstrap-del-error-estandar.html"><a href="el-estimador-bootstrap-del-error-estandar.html#mas-alla-de-muestras-aleatorias-simples"><i class="fa fa-check"></i>Más alla de muestras aleatorias simples</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="intervalos-de-confianza.html"><a href="intervalos-de-confianza.html"><i class="fa fa-check"></i><b>6.3</b> Intervalos de confianza</a></li>
<li class="chapter" data-level="6.4" data-path="bootstrap-en-r.html"><a href="bootstrap-en-r.html"><i class="fa fa-check"></i><b>6.4</b> Bootstrap en R</a></li>
<li class="chapter" data-level="6.5" data-path="conclusiones-y-observaciones.html"><a href="conclusiones-y-observaciones.html"><i class="fa fa-check"></i><b>6.5</b> Conclusiones y observaciones</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="teoria-basica-de-simulacion.html"><a href="teoria-basica-de-simulacion.html"><i class="fa fa-check"></i><b>7</b> Teoría básica de simulación</a><ul>
<li class="chapter" data-level="7.1" data-path="numeros-pseudoaleatorios.html"><a href="numeros-pseudoaleatorios.html"><i class="fa fa-check"></i><b>7.1</b> Números pseudoaleatorios</a><ul>
<li class="chapter" data-level="" data-path="numeros-pseudoaleatorios.html"><a href="numeros-pseudoaleatorios.html#generadores-congruenciales-y-mersenne-twister"><i class="fa fa-check"></i>Generadores congruenciales y Mersenne-Twister</a></li>
<li class="chapter" data-level="" data-path="numeros-pseudoaleatorios.html"><a href="numeros-pseudoaleatorios.html#pruebas-de-aleatoriedad"><i class="fa fa-check"></i>Pruebas de aleatoriedad</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="variables-aleatorias-1.html"><a href="variables-aleatorias-1.html"><i class="fa fa-check"></i><b>7.2</b> Variables aleatorias</a><ul>
<li class="chapter" data-level="" data-path="variables-aleatorias-1.html"><a href="variables-aleatorias-1.html#familias-discretas-importantes"><i class="fa fa-check"></i>Familias discretas importantes</a></li>
<li class="chapter" data-level="" data-path="variables-aleatorias-1.html"><a href="variables-aleatorias-1.html#familias-continuas-importantes"><i class="fa fa-check"></i>Familias Continuas importantes</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="simulacion-de-variables-aleatorias.html"><a href="simulacion-de-variables-aleatorias.html"><i class="fa fa-check"></i><b>7.3</b> Simulación de variables aleatorias</a><ul>
<li class="chapter" data-level="" data-path="simulacion-de-variables-aleatorias.html"><a href="simulacion-de-variables-aleatorias.html#variables-aletaorias-discretas"><i class="fa fa-check"></i>Variables aletaorias discretas</a></li>
<li class="chapter" data-level="" data-path="simulacion-de-variables-aleatorias.html"><a href="simulacion-de-variables-aleatorias.html#aceptacion-y-rechazo"><i class="fa fa-check"></i>Aceptación y rechazo</a></li>
<li class="chapter" data-level="" data-path="simulacion-de-variables-aleatorias.html"><a href="simulacion-de-variables-aleatorias.html#variables-aleatorias-continuas-1"><i class="fa fa-check"></i>Variables aleatorias continuas</a></li>
<li class="chapter" data-level="" data-path="simulacion-de-variables-aleatorias.html"><a href="simulacion-de-variables-aleatorias.html#aceptacion-y-rechazo-1"><i class="fa fa-check"></i>Aceptación y rechazo</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="simulacion-de-modelos.html"><a href="simulacion-de-modelos.html"><i class="fa fa-check"></i><b>8</b> Simulación de modelos</a><ul>
<li class="chapter" data-level="" data-path="simulacion-de-modelos.html"><a href="simulacion-de-modelos.html#para-que-simular-de-un-modelo"><i class="fa fa-check"></i>¿Para qué simular de un modelo?</a></li>
<li class="chapter" data-level="8.1" data-path="distribuciones-multivariadas.html"><a href="distribuciones-multivariadas.html"><i class="fa fa-check"></i><b>8.1</b> Distribuciones multivariadas</a><ul>
<li class="chapter" data-level="" data-path="distribuciones-multivariadas.html"><a href="distribuciones-multivariadas.html#regla-de-bayes"><i class="fa fa-check"></i>Regla de Bayes</a></li>
<li class="chapter" data-level="" data-path="distribuciones-multivariadas.html"><a href="distribuciones-multivariadas.html#independencia"><i class="fa fa-check"></i>Independencia</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="modelos-graficos-y-simulacion-predictiva.html"><a href="modelos-graficos-y-simulacion-predictiva.html"><i class="fa fa-check"></i><b>8.2</b> Modelos gráficos y simulación predictiva</a></li>
<li class="chapter" data-level="8.3" data-path="inferencia-visual.html"><a href="inferencia-visual.html"><i class="fa fa-check"></i><b>8.3</b> Inferencia visual</a><ul>
<li class="chapter" data-level="" data-path="inferencia-visual.html"><a href="inferencia-visual.html#inferencia"><i class="fa fa-check"></i>Inferencia</a></li>
<li class="chapter" data-level="" data-path="inferencia-visual.html"><a href="inferencia-visual.html#protocolos-de-inferencia-visual"><i class="fa fa-check"></i>Protocolos de inferencia visual</a></li>
<li class="chapter" data-level="" data-path="inferencia-visual.html"><a href="inferencia-visual.html#pruebas-de-hipotesis-tipicas"><i class="fa fa-check"></i>Pruebas de hipótesis típicas</a></li>
<li class="chapter" data-level="" data-path="inferencia-visual.html"><a href="inferencia-visual.html#inferencia-visual-1"><i class="fa fa-check"></i>Inferencia visual</a></li>
<li class="chapter" data-level="" data-path="inferencia-visual.html"><a href="inferencia-visual.html#mas-alla-que-permutacion"><i class="fa fa-check"></i>Más allá que permutación</a></li>
<li class="chapter" data-level="" data-path="inferencia-visual.html"><a href="inferencia-visual.html#otras-consideraciones-1"><i class="fa fa-check"></i>Otras consideraciones</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="simulacion-para-calculo-de-tamano-de-muestrapoder-estadistico.html"><a href="simulacion-para-calculo-de-tamano-de-muestrapoder-estadistico.html"><i class="fa fa-check"></i><b>8.4</b> Simulación para cálculo de tamaño de muestra/poder estadístico</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="inferencia-parametrica.html"><a href="inferencia-parametrica.html"><i class="fa fa-check"></i><b>9</b> Inferencia paramétrica</a><ul>
<li class="chapter" data-level="9.1" data-path="maxima-verosimilitud.html"><a href="maxima-verosimilitud.html"><i class="fa fa-check"></i><b>9.1</b> Máxima verosimilitud</a><ul>
<li class="chapter" data-level="" data-path="maxima-verosimilitud.html"><a href="maxima-verosimilitud.html#propiedades-de-los-estimadores-de-maxima-verosimilitud"><i class="fa fa-check"></i>Propiedades de los estimadores de máxima verosimilitud</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="bootstrap-parametrico.html"><a href="bootstrap-parametrico.html"><i class="fa fa-check"></i><b>9.2</b> Bootstrap paramétrico</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="analisis-bayesiano.html"><a href="analisis-bayesiano.html"><i class="fa fa-check"></i><b>10</b> Análisis bayesiano</a><ul>
<li class="chapter" data-level="10.1" data-path="probabilidad-subjetiva.html"><a href="probabilidad-subjetiva.html"><i class="fa fa-check"></i><b>10.1</b> Probabilidad subjetiva</a></li>
<li class="chapter" data-level="10.2" data-path="regla-de-bayes-e-inferencia-bayesiana.html"><a href="regla-de-bayes-e-inferencia-bayesiana.html"><i class="fa fa-check"></i><b>10.2</b> Regla de Bayes e inferencia bayesiana</a><ul>
<li class="chapter" data-level="" data-path="regla-de-bayes-e-inferencia-bayesiana.html"><a href="regla-de-bayes-e-inferencia-bayesiana.html#regla-de-bayes-en-modelos-y-datos"><i class="fa fa-check"></i>Regla de Bayes en modelos y datos</a></li>
<li class="chapter" data-level="" data-path="regla-de-bayes-e-inferencia-bayesiana.html"><a href="regla-de-bayes-e-inferencia-bayesiana.html#objetivos-de-la-inferencia"><i class="fa fa-check"></i>Objetivos de la inferencia</a></li>
<li class="chapter" data-level="" data-path="regla-de-bayes-e-inferencia-bayesiana.html"><a href="regla-de-bayes-e-inferencia-bayesiana.html#calculo-de-la-distribucion-posterior"><i class="fa fa-check"></i>Cálculo de la distribución posterior</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="distribuciones-conjugadas.html"><a href="distribuciones-conjugadas.html"><i class="fa fa-check"></i><b>10.3</b> Distribuciones conjugadas</a><ul>
<li class="chapter" data-level="" data-path="distribuciones-conjugadas.html"><a href="distribuciones-conjugadas.html#ejemplo-bernoulli"><i class="fa fa-check"></i>Ejemplo: Bernoulli</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="aproximacion-por-cuadricula.html"><a href="aproximacion-por-cuadricula.html"><i class="fa fa-check"></i><b>10.4</b> Aproximación por cuadrícula</a></li>
<li class="chapter" data-level="10.5" data-path="mcmc.html"><a href="mcmc.html"><i class="fa fa-check"></i><b>10.5</b> MCMC</a><ul>
<li class="chapter" data-level="" data-path="mcmc.html"><a href="mcmc.html#introduccion-metropolis"><i class="fa fa-check"></i>Introducción Metrópolis</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="metropolis.html"><a href="metropolis.html"><i class="fa fa-check"></i><b>10.6</b> Metrópolis</a><ul>
<li class="chapter" data-level="" data-path="metropolis.html"><a href="metropolis.html#inferencia-de-dos-proporciones-binomiales"><i class="fa fa-check"></i>Inferencia de dos proporciones binomiales</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="muestreador-de-gibbs.html"><a href="muestreador-de-gibbs.html"><i class="fa fa-check"></i><b>10.7</b> Muestreador de Gibbs</a><ul>
<li class="chapter" data-level="" data-path="muestreador-de-gibbs.html"><a href="muestreador-de-gibbs.html#conclusiones-y-observaciones-metropolis-y-gibbs"><i class="fa fa-check"></i>Conclusiones y observaciones Metrópolis y Gibbs</a></li>
</ul></li>
<li class="chapter" data-level="10.8" data-path="jags.html"><a href="jags.html"><i class="fa fa-check"></i><b>10.8</b> JAGS</a><ul>
<li class="chapter" data-level="" data-path="jags.html"><a href="jags.html#ejemplo-normal-1"><i class="fa fa-check"></i>Ejemplo normal</a></li>
</ul></li>
<li class="chapter" data-level="10.9" data-path="diagnosticos.html"><a href="diagnosticos.html"><i class="fa fa-check"></i><b>10.9</b> Diagnósticos</a><ul>
<li class="chapter" data-level="" data-path="diagnosticos.html"><a href="diagnosticos.html#recomendaciones-generales"><i class="fa fa-check"></i>Recomendaciones generales</a></li>
</ul></li>
<li class="chapter" data-level="10.10" data-path="hmc-y-stan.html"><a href="hmc-y-stan.html"><i class="fa fa-check"></i><b>10.10</b> HMC y Stan</a><ul>
<li class="chapter" data-level="" data-path="hmc-y-stan.html"><a href="hmc-y-stan.html#muestreo-hmc"><i class="fa fa-check"></i>Muestreo HMC</a></li>
<li class="chapter" data-level="" data-path="hmc-y-stan.html"><a href="hmc-y-stan.html#iniciales"><i class="fa fa-check"></i>Iniciales</a></li>
<li class="chapter" data-level="" data-path="hmc-y-stan.html"><a href="hmc-y-stan.html#recursos-adicionales-de-stan"><i class="fa fa-check"></i>Recursos adicionales de Stan</a></li>
</ul></li>
<li class="chapter" data-level="10.11" data-path="modelos-jerarquicos.html"><a href="modelos-jerarquicos.html"><i class="fa fa-check"></i><b>10.11</b> Modelos jerárquicos</a><ul>
<li class="chapter" data-level="" data-path="modelos-jerarquicos.html"><a href="modelos-jerarquicos.html#modelo-jerarquico-una-moneda"><i class="fa fa-check"></i>Modelo jerárquico una moneda</a></li>
<li class="chapter" data-level="" data-path="modelos-jerarquicos.html"><a href="modelos-jerarquicos.html#multiples-monedas-de-una-misma-fabrica"><i class="fa fa-check"></i>Multiples monedas de una misma fábrica</a></li>
<li class="chapter" data-level="" data-path="modelos-jerarquicos.html"><a href="modelos-jerarquicos.html#ejemplo-estimacion-de-tasas-de-mortalidad"><i class="fa fa-check"></i>Ejemplo: estimación de tasas de mortalidad</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="tareas.html"><a href="tareas.html"><i class="fa fa-check"></i>Tareas</a><ul>
<li class="chapter" data-level="" data-path="transformacion-de-datos-1.html"><a href="transformacion-de-datos-1.html"><i class="fa fa-check"></i>2-Transformación de datos</a></li>
<li class="chapter" data-level="" data-path="datos-limpios-1.html"><a href="datos-limpios-1.html"><i class="fa fa-check"></i>3-Datos Limpios</a></li>
<li class="chapter" data-level="" data-path="probabilidad.html"><a href="probabilidad.html"><i class="fa fa-check"></i>4-Probabilidad</a></li>
<li class="chapter" data-level="" data-path="bootstrap.html"><a href="bootstrap.html"><i class="fa fa-check"></i>5-Bootstrap</a><ul>
<li class="chapter" data-level="" data-path="bootstrap.html"><a href="bootstrap.html#solucion"><i class="fa fa-check"></i>Solución</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="cobertura-de-intervalos-de-confianza.html"><a href="cobertura-de-intervalos-de-confianza.html"><i class="fa fa-check"></i>6-Cobertura de intervalos de confianza</a><ul>
<li class="chapter" data-level="" data-path="cobertura-de-intervalos-de-confianza.html"><a href="cobertura-de-intervalos-de-confianza.html#solucion-1"><i class="fa fa-check"></i>Solución</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="simulacion-de-modelos-1.html"><a href="simulacion-de-modelos-1.html"><i class="fa fa-check"></i>7-Simulación de modelos</a></li>
<li class="chapter" data-level="" data-path="simulacion-de-modelos-de-regresion.html"><a href="simulacion-de-modelos-de-regresion.html"><i class="fa fa-check"></i>8-Simulación de modelos de regresión</a></li>
<li class="chapter" data-level="" data-path="inferencia-grafica-tamano-de-muestra-bootstrap-parametrico-.html"><a href="inferencia-grafica-tamano-de-muestra-bootstrap-parametrico-.html"><i class="fa fa-check"></i>9-Inferencia gráfica, tamaño de muestra, bootstrap paramétrico.</a><ul>
<li class="chapter" data-level="" data-path="inferencia-grafica-tamano-de-muestra-bootstrap-parametrico-.html"><a href="inferencia-grafica-tamano-de-muestra-bootstrap-parametrico-.html#solucion-2"><i class="fa fa-check"></i>Solución</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="familias-conjugadas.html"><a href="familias-conjugadas.html"><i class="fa fa-check"></i>10-Familias conjugadas</a></li>
<li class="chapter" data-level="" data-path="metropolis-2.html"><a href="metropolis-2.html"><i class="fa fa-check"></i>11-Metropolis</a><ul>
<li class="chapter" data-level="" data-path="metropolis-2.html"><a href="metropolis-2.html#solucion-3"><i class="fa fa-check"></i>Solución</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="mcmc-convergencia.html"><a href="mcmc-convergencia.html"><i class="fa fa-check"></i>12-MCMC convergencia</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="referencias.html"><a href="referencias.html"><i class="fa fa-check"></i>Referencias</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Publicado con bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Estadística Computacional</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regla-de-bayes-e-inferencia-bayesiana" class="section level2">
<h2><span class="header-section-number">10.2</span> Regla de Bayes e inferencia bayesiana</h2>
<p>Thomas Bayes (1702-1761) fue un matemático y ministro de la iglesia
presbiteriana, en 1764 se publicó su famoso teorema.</p>
<p>Una aplicación crucial de la regla de Bayes es determinar la probabilidad de un
modelo dado un conjunto de datos. Lo que el modelo determina es la probabilidad
de los datos condicional a valores particulares de los parámetros y a la
estructura del modelo. Por su parte usamos la regla de Bayes para ir de la
probabilidad de los datos, dado el modelo, a la probabilidad del modelo, dados
los datos.</p>
<div id="ejemplo-lanzamientos-de-monedas" class="section level4 unnumbered">
<h4>Ejemplo: Lanzamientos de monedas</h4>
<p>Comencemos recordando la regla de Bayes usando dos variables aleatorias
discretas. Lanzamos una moneda 3 veces, sea <span class="math inline">\(X\)</span> la variable aleatoria
correspondiente al número de Águilas observadas y <span class="math inline">\(Y\)</span> registra el número de
cambios entre águilas y soles.</p>
<ul>
<li><p>Escribe la distribución conjunta de las variables, y las distribuciones
marginales.</p></li>
<li><p>Considera la probabilidad de observar un cambio condicional a que observamos
un águila y compara con la probabilidad de observar un águila condicional a
que observamos un cambio.</p></li>
</ul>
<p>Para entender probabilidad condicional podemos pensar en restringir nuestra
atención a una única fila o columna de la tabla. Supongamos que alguien lanza
una moneda 3 veces y nos informa que
la secuencia contiene exactamente un cambio. Dada esta información podemos
restringir nuestra atención a la fila correspondiente a un solo cambio. Sabemos
que ocurrió uno de los eventos de esa fila. Las probabilidades relativas
de los eventos de esa fila no han cambiado pero sabemos que la probabilidad
total debe sumar uno, por lo que simplemente normalizamos dividiendo entre
<span class="math inline">\(p(C=1)\)</span>. En este ejemplo vemos que cuando no sabemos nada acerca del número de
cambios, todo lo que sabemos de número de águilas está contenido en la
distribución marginal de <span class="math inline">\(X\)</span>, por otro lado, si sabemos que hubo un cambio
entonces sabemos que estamos en los escenarios de la fila correspondiente a un
cambio, y calculamos estas probabilidades condicionales. Es así que nos movemos
de creencias iniciales (marginal) acerca de <span class="math inline">\(X\)</span> a creecnias posteriores
(condicional).</p>
</div>
<div id="regla-de-bayes-en-modelos-y-datos" class="section level3 unnumbered">
<h3>Regla de Bayes en modelos y datos</h3>
<p>Una de las aplicaciones más importantes de la regla de Bayes es cuando las
variables fila y columna son datos y parámetros del modelo respectivamente.</p>
<p>Un modelo especifica la probabilidad de valores particulares dado la estructura
del modelo y valores de los parámetros. Por ejemplo en un modelo de lanzamientos
de monedas tenemos
<span class="math display">\[p(x = A|\theta)=\theta\]</span>,</p>
<p><span class="math display">\[p(x = S|\theta)= 1- \theta\]</span></p>
<p>De manera general, el modelo especifica:</p>
<p><span class="math display">\[p(\text{datos}|\text{valores de parámetros y estructura del modelo})\]</span></p>
<p>y usamos la regla de Bayes para convertir la expresión anterior a lo que
nos interesa de verdad, que es, que tanta certidumbre tenemos del modelo
condicional a los datos:</p>
<p><span class="math display">\[p(\text{valores de parámetros y estructura del modelo} | \text{datos})\]</span></p>
<p>Una vez que observamos los datos, usamos la regla de Bayes para determinar
o actualizar nuestras creencias de posibles parámetros y modelos.</p>
<p>Entonces:</p>
<div class="caja">
<ul>
<li><p>Cuantificamos la información (o incertidumbre) acerca del parámetro
desconocido <span class="math inline">\(\theta\)</span> mediante distribuciones de probabilidad.</p></li>
<li><p>Antes de observar datos <span class="math inline">\(x\)</span>, cuantificamos la información de <span class="math inline">\(\theta\)</span> externa
a <span class="math inline">\(x\)</span> en una <strong>distribución a priori</strong>:</p></li>
</ul>
<p><span class="math display">\[p(\theta),\]</span>
esto es, la distribución a priori resume nuestras creencias acerca del parámetro
ajenas a los datos. Por
otra parte, cuantificamos la información de <span class="math inline">\(\theta\)</span> asociada a <span class="math inline">\(x\)</span> mediante la
<strong>distribución de verosimilitud</strong></p>
<p><span class="math display">\[p(x|\theta)\]</span></p>
<ul>
<li>Combinamos la información a priori y la información que provee <span class="math inline">\(x\)</span> mediante el
<strong>teorema de Bayes</strong> obteniendo así la <strong>distribución posterior</strong></li>
</ul>
<p><span class="math display">\[p(\theta|x) \propto p(x|\theta)p(\theta).\]</span></p>
<ul>
<li>Las inferencias se obtienen de resúmenes de la distribución posterior.</li>
</ul>
</div>
<div id="ejemplo-ingesta-calorica-en-estudiantes" class="section level4 unnumbered">
<h4>Ejemplo: Ingesta calórica en estudiantes</h4>
<p>Supongamos que nos interesa aprender los hábitos alimenticios de los estudiantes
universitarios en México, y escuchamos que de acuerdo a investigaciones se
recomienda que un adulto promedio ingiera 2500 kcal. Es así que buscamos conocer
que proporción de los estudiantes siguen esta recomendación, para ello
tomaremos una muestra aleatoria de estudiantes del ITAM. Denotemos por <span class="math inline">\(\theta\)</span>
la proporción de estudiantes que ingieren en un día 2500 kcal o más. El valor
de <span class="math inline">\(\theta\)</span> es desconocido, y desde el punto de vista bayesiano cuando tenemos
incertidumbre de algo (puede ser un parámetro o una predicción) lo vemos como
una variable aleatoria y por tanto tiene asociada una distribución de
probabilidad que actualizaremos conforme obtenemos información (observamos
datos).</p>
<p>Recordemos que la distribución <span class="math inline">\(p(\theta)\)</span> se conoce como la distribución
<em>a priori</em> y representa nuestras creencias de los posibles valores que puede
tomar el parámetro. Supongamos que tras leer artículos y entrevistar
especialistas consideramos los posibles valores de <span class="math inline">\(\theta\)</span> y les asigmanos
pesos:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
theta &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>, <span class="fl">0.1</span>)
pesos.prior &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="fl">5.2</span>, <span class="dv">8</span>, <span class="fl">7.2</span>, <span class="fl">4.6</span>, <span class="fl">2.1</span>, <span class="fl">0.7</span>, <span class="fl">0.1</span>, <span class="dv">0</span>, <span class="dv">0</span>)
prior &lt;-<span class="st"> </span>pesos.prior<span class="op">/</span><span class="kw">sum</span>(pesos.prior) 
prior_df &lt;-<span class="st"> </span><span class="kw">data_frame</span>(theta, <span class="dt">prior =</span> <span class="kw">round</span>(prior, <span class="dv">3</span>))
prior_df
<span class="co">#&gt; # A tibble: 10 x 2</span>
<span class="co">#&gt;    theta prior</span>
<span class="co">#&gt;    &lt;dbl&gt; &lt;dbl&gt;</span>
<span class="co">#&gt;  1  0.05 0.035</span>
<span class="co">#&gt;  2  0.15 0.18 </span>
<span class="co">#&gt;  3  0.25 0.277</span>
<span class="co">#&gt;  4  0.35 0.249</span>
<span class="co">#&gt;  5  0.45 0.159</span>
<span class="co">#&gt;  6  0.55 0.073</span>
<span class="co">#&gt;  7  0.65 0.024</span>
<span class="co">#&gt;  8  0.75 0.003</span>
<span class="co">#&gt;  9  0.85 0    </span>
<span class="co">#&gt; 10  0.95 0</span></code></pre>
<p>Una vez que cuantificamos nuestro conocimiento (o la falta de este) sobre los
posibles valores que puede tomar <span class="math inline">\(\theta\)</span> especificamos la verosimilitud y la
distribución conjunta <span class="math inline">\(p(x, \theta)\)</span>, donde <span class="math inline">\(x = (x_1,...,x_N)\)</span> veamos
la distribución de un estudiante en particular:
<span class="math display">\[p(x_i|\theta) \sim Bernoulli(\theta),\]</span>
para <span class="math inline">\(i=1,...,N\)</span>, es decir, condicional a <span class="math inline">\(\theta\)</span> la probabilidad de que un
estudiante ingiera más de 2500 calorías es <span class="math inline">\(\theta\)</span> y la función de
verosimilitud <span class="math inline">\(p(x_1,...,x_N|\theta) = \mathcal{L}(\theta)\)</span>:</p>
<p><span class="math display">\[p(x_1,...,x_N|\theta) = \prod_{n=1}^N p(x_n|\theta)\]</span>
<span class="math display">\[= \theta^z(1 - \theta)^{N-z}\]</span></p>
<p>donde <span class="math inline">\(z\)</span> denota el número de estudiantes que ingirió al menos 2500 kcal y <span class="math inline">\(N-z\)</span>
el número de estudiantes que ingirió menos de 2500 kcal.</p>
<p>Ahora calculamos la distribución posterior de <span class="math inline">\(\theta\)</span>
usando la regla de Bayes:</p>
<p><span class="math display">\[p(\theta|x) = \frac{p(x_1,...,x_N,\theta)}{p(x)}\]</span>
<span class="math display">\[\propto  p(\theta)\mathcal{L}(\theta)\]</span></p>
<p>Vemos que la distribución posterior es proporcional al producto de la
verosimilitud y la distribución inicial, el denominador <span class="math inline">\(p(x)\)</span> no depende de
<span class="math inline">\(\theta\)</span> por lo que es constante (como función de <span class="math inline">\(\theta\)</span>) y esta ahí para
normalizar la distribución posterior asegurando que tengamos una distribución
de probabilidad.</p>
</div>
<div id="inicial-discreta" class="section level4 unnumbered">
<h4>Inicial discreta</h4>
<p>Volviendo a nuestro ejemplo, usamos la inicial discreta que discutimos (tabla
de pesos normalizados) y supongamos que tomamos una muestra de 30 alumnos de
los cuales <span class="math inline">\(z=11\)</span> ingieren al menos 2500 kcal, calculemos la distribución
posterior de <span class="math inline">\(\theta\)</span>, usando que</p>
<p><span class="math display">\[\mathcal{L}(\theta) = \theta^{z}(1-\theta)^{N-z}\]</span>
con <span class="math inline">\(0&lt;\theta&lt;1\)</span></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(LearnBayes)

N &lt;-<span class="st"> </span><span class="dv">30</span> <span class="co"># estudiantes</span>
z &lt;-<span class="st"> </span><span class="dv">11</span> <span class="co"># éxitos</span>

<span class="co"># Verosimilitud</span>
Like &lt;-<span class="st"> </span>theta <span class="op">^</span><span class="st"> </span>z <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>theta) <span class="op">^</span><span class="st"> </span>(N <span class="op">-</span><span class="st"> </span>z)
product &lt;-<span class="st"> </span>Like <span class="op">*</span><span class="st"> </span>prior

<span class="co"># Distribución posterior (normalizamos)</span>
post &lt;-<span class="st"> </span>product <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(product)

dists &lt;-<span class="st"> </span><span class="kw">bind_cols</span>(prior_df, <span class="dt">post =</span> post)
<span class="kw">round</span>(dists, <span class="dv">3</span>)
<span class="co">#&gt; # A tibble: 10 x 3</span>
<span class="co">#&gt;    theta prior  post</span>
<span class="co">#&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;</span>
<span class="co">#&gt;  1  0.05 0.035 0    </span>
<span class="co">#&gt;  2  0.15 0.18  0.006</span>
<span class="co">#&gt;  3  0.25 0.277 0.22 </span>
<span class="co">#&gt;  4  0.35 0.249 0.529</span>
<span class="co">#&gt;  5  0.45 0.159 0.224</span>
<span class="co">#&gt;  6  0.55 0.073 0.021</span>
<span class="co">#&gt;  7  0.65 0.024 0    </span>
<span class="co">#&gt;  8  0.75 0.003 0    </span>
<span class="co">#&gt;  9  0.85 0     0    </span>
<span class="co">#&gt; 10  0.95 0     0</span>

<span class="co"># También podemos usar la función pdisc</span>
<span class="kw">pdisc</span>(<span class="dt">p =</span> theta, <span class="dt">prior =</span> prior, <span class="dt">data =</span> <span class="kw">c</span>(z, N <span class="op">-</span><span class="st"> </span>z)) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">round</span>(<span class="dv">3</span>)
<span class="co">#&gt;  [1] 0.000 0.006 0.220 0.529 0.224 0.021 0.000 0.000 0.000 0.000</span>

<span class="co"># Alargamos los datos para graficar</span>
dists_l &lt;-<span class="st"> </span>dists <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">gather</span>(dist, val, prior<span class="op">:</span>post) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">dist =</span> <span class="kw">factor</span>(dist, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;prior&quot;</span>, <span class="st">&quot;post&quot;</span>)))

<span class="kw">ggplot</span>(dists_l, <span class="kw">aes</span>(<span class="dt">x =</span> theta, <span class="dt">y =</span> val)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">stat =</span> <span class="st">&quot;identity&quot;</span>, <span class="dt">fill =</span> <span class="st">&quot;darkgray&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>dist) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="kw">expression</span>(theta), <span class="dt">y =</span> <span class="kw">expression</span>(<span class="kw">p</span>(theta))) </code></pre>
<p><img src="09-analisis_bayesiano_files/figure-html/unnamed-chunk-2-1.png" width="480" /></p>
<p><img src="imagenes/manicule2.jpg" /> ¿Cómo se ve la distribución posterior si tomamos
una muestra de tamaño 90 donde observamos la misma proporción de éxitos.
Realiza los cálculos y graficala como un panel adicional de la gráfica
anterior.</p>
<ul>
<li>¿Cómo definirías la distribución inicial si no tuvieras conocimiento de los
artículos y expertos?</li>
</ul>
</div>
<div id="evidencia" class="section level4 unnumbered">
<h4>Evidencia</h4>
<p>Ahora, en el teorema de Bayes también encontramos el término <span class="math inline">\(p(x)\)</span> que
denominamos la <strong>evidencia</strong>, también se conoce como verosimilitud
marginal o inicial predictiva.
<span class="math display">\[p(\theta|x)=\frac{p(x|\theta)p(\theta)}{p(x)}\]</span></p>
<p>La evidencia es la probabilidad de los datos de acuerdo al modelo y se calcula
sumando a lo largo de todos los posibles valores de los parámetros y ponderando
por nuestra certidumbre en esos valores de los parámetros.</p>
<p>Es importante notar que hablamos de valores de los parámetros <span class="math inline">\(\theta\)</span>
únicamente en el contexto de un modelo particular pues este el que da sentido a
los parámetros. Podemos hacer evidente el modelo en la notación,</p>
<p><span class="math display">\[p(\theta|x,M)=\frac{p(x|\theta,M)p(\theta|M)}{p(x|M)}\]</span></p>
<p>en este contexto la evidencia se define como:</p>
<p><span class="math display">\[p(x|M)=\int p(x|\theta,M)p(\theta|M)d\theta\]</span></p>
<p>La notación anterior es conveniente sobre todo cuando estamos considerando
más de un modelo y queremos usar los datos para determinar la certeza que
tenemos en cada modelo. Supongamos que tenemos dos modelos <span class="math inline">\(M_1\)</span> y <span class="math inline">\(M_2\)</span>,
entonces podemos calcular el cociente de <span class="math inline">\(p(M_1|x)\)</span> y <span class="math inline">\(p(M_2|x)\)</span> obteniendo:</p>
<p><span class="math display">\[\frac{p(M_1|x)}{p(M_2|x)} = \frac{p(x|M_1) \cdot p(M_1)}{p(x|M_2)\cdot p(M_2)}\]</span></p>
<p>El cociente de evidencia <span class="math inline">\(\frac{p(x|M_1)}{p(x|M_2)}\)</span> se conoce como <strong>factor de
Bayes</strong>, y <span class="math inline">\(p(M_i)\)</span> describe nuestras creencias iniciales en cada modelo.</p>
</div>
<div id="invarianza-en-el-orden-de-los-datos" class="section level4 unnumbered">
<h4>Invarianza en el orden de los datos</h4>
<p>Vimos que la regla de Bayes nos permite pasar del conocimiento inicial
<span class="math inline">\(p(\theta)\)</span> al posterior <span class="math inline">\(p(\theta|x)\)</span> conforme recopilamos datos. Supongamos
ahora que observamos
más datos, los denotamos <span class="math inline">\(x&#39;\)</span>, podemos volver a actualizar nuestras creencias
pasando de <span class="math inline">\(p(\theta|x)\)</span> a <span class="math inline">\(p(\theta|x,x&#39;)\)</span>. Entonces podemos preguntarnos si
nuestro conocimiento posterior cambia si actualizamos de acuerdo a <span class="math inline">\(x\)</span> primero
y después <span class="math inline">\(x&#39;\)</span> o vice-versa. La respuesta es que si <span class="math inline">\(p(x|\theta)\)</span> y
<span class="math inline">\(p(x&#39;|\theta)\)</span> son <em>iid</em> entonces el orden en que actualizamos nuestro
conocimiento no afecta la distribución posterior.</p>
<p>La <strong>invarianza al orden</strong> tiene sentido intuitivamente: Si la función de
verosimilitud no depende del tiempo o del ordenamineto de los datos, entonces
la posterior tampoco tiene porque depender del ordenamiento de los datos.</p>
<!--
#### Pasos de un análisis de datos bayesiano

Como vimos en los ejemplos, en general un análisis de datos bayesiano sigue los
siguientes pasos:

1. Identificar los datos releventes a nuestra pregunta de investigación, el tipo 
de datos que vamos a describir, que variables se van a predecir, que variables 

2. Definir el modelo descriptivo para los datos. La forma matemática y los 
parámetros deben ser apropiados para los objetivos del análisis.

3. Especificar la distribución inicial de los parámetros.

4. Utilizar inferencia bayesiana para reubicar la credibilidad a lo largo de 
los posibles valores de los parámetros.

5. Verificar que la distribución posterior replique los datos de manera 
razonable, de no ser el caso considerar otros modelos descriptivos para los datos.

En el caso de un volado, los datos consisten en águilas y soles, y para el 
modelo descriptivo necesitamos una expresión del función de verosimilitud:

$p(x|\theta)=\theta^x(1-\theta)^{1-x}$

esto es, $x$ tiene una distribución $Bernoulli(\theta)$.

El siguiente paso es determinar la distribución inicial sobre el espacio de
parámetros, como ejemplo, supongamos que solo hay 3 valores de $\theta$ que consideramos $\theat \in \{(0.25, 0.5, 0.75)\}$, y asignamos las probabilidades
$p(\theta = 0.25) = 0.25$, $p(\theta = 0.5) = 0.5$ y $p(\theta = 0.75) = 0.25$


```r
theta <- c(0.25, 0.5, 0.75)
prior <- data_frame(theta, p = c(0.25, 0.5, 0.25))
```

El siguiente paso es recolectar los datos y utilizar la regla de Bayes para 
reubicar nuestras creencias a lo largo de los posibles valores. Supongamos 
que observamos un único volado y resulta en águila.

-->
<pre class="sourceCode r"><code class="sourceCode r">Like &lt;-<span class="st"> </span>theta <span class="op">^</span><span class="st"> </span><span class="dv">1</span> <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>theta) <span class="op">^</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)
product &lt;-<span class="st"> </span>Like <span class="op">*</span><span class="st"> </span>prior<span class="op">$</span>p
post &lt;-<span class="st"> </span>product <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(product)
post
<span class="co">#&gt; [1] 0.125 0.500 0.375</span></code></pre>
</div>
</div>
<div id="objetivos-de-la-inferencia" class="section level3 unnumbered">
<h3>Objetivos de la inferencia</h3>
<p>Los tres objetivos de la inferencia son: estimación de parámetros, predicción
de valores y comparación de modelos.</p>
<ol style="list-style-type: decimal">
<li>La <strong>estimación de parámetros</strong> implica determinar hasta que punto creemos en
cada posible valor del parámetro. En estadística bayesiana la estimación se
realiza con la distribución posterior sobre los valores
de los parámetros <span class="math inline">\(\theta\)</span>.<br />
La siguiente gráfica ejemplifica un experimento Bernoulli, con dos posibles
iniciales, los datos observados son <span class="math inline">\(N=20\)</span> lanzamientos de moneda que resultan
en <span class="math inline">\(12\)</span> éxitos o águilas.</li>
</ol>
<p><img src="09-analisis_bayesiano_files/figure-html/unnamed-chunk-5-1.png" width="700.8" /><img src="09-analisis_bayesiano_files/figure-html/unnamed-chunk-5-2.png" width="700.8" /></p>
<ol start="2" style="list-style-type: decimal">
<li><strong>Predicción</strong> de valores. Usando nuestro conocimiento actual nos interesa
predecir la probabilidad de datos futuros. La probabilidad predictiva de un
dato <span class="math inline">\(y\)</span> (no observado) se determina promediando las probabilidades predictivas
de los datos a lo largo de todos los posibles valores de los parámetros y
ponderados por la creencia en los valores de los parámetros. Cuando solo
contamos con nuestro conocimiento incial tendríamos:
<span class="math display">\[p(y) =\int p(y|\theta)p(\theta)d\theta\]</span>
Notese que la ecuación anterior coincide con la correspondiente a la evidencia,
con la diferencia de que la evidencia se refiere a un valor observado y en
esta ecuación estamos calculando la probabilidad de cualquier valor <span class="math inline">\(y\)</span>.<br />
Una vez que obserbamos datos tenemos la distribución predictiva posterior:
<span class="math display">\[p(y|x) =\int p(y|\theta)p(\theta|x)d\theta\]</span>
Por ejemplo podemos usar las creencias iniciales del modelo 1, que propusimos
arriba para calcular la probabilidad predictiva de observar águila:</li>
</ol>
<p><span class="math display">\[p(y=S) = \sum_{\theta}p(y=A|\theta)p(\theta) = 0.5\]</span></p>
<p>Vale la pena destacar que las prediciones son probabilidades de cada posible
valor condicional a nuestro modelo de creencias actuales. Si nos interesa
predecir un valor particular en lugar de una distribución a lo largo de todos
los posibles valores podemos usar la media de la distribución predictiva. Por
tanto el valor a predecir sería:</p>
<p><span class="math display">\[p(y)=\int y p(y) dy\]</span></p>
<p>La integral anterior únicamente tiene sentido si <span class="math inline">\(y\)</span> es una variable continua.
Si <span class="math inline">\(y\)</span> es nominal, como el resultado de un volado, entonces podemos usar el
valor más probable.</p>
<!--
![](imagenes/manicule2.jpg) Calcula las probabilidades predictivas usando 
la distribución posterior de cada modelo. ¿Cuál sería tu predicción?
-->
<ol start="3" style="list-style-type: decimal">
<li><strong>Comparación de modelos</strong>, una caracterítica conveniente de la comparación
de modelos en estadística bayesiana es que la complejidad del modelo se toma
en cuenta de manera automática.</li>
</ol>
<p>Recordemos los dos modelos discretos, en el primero supusimos que el parámetro
<span class="math inline">\(\theta\)</span> únicamente puede tomar uno de 3 valores (0.25, 0.5, 0.75), esta
restricción dió lugar a un modelo simple. Por su parte, el modelo 2 es más
complejo y permite muchos más valores de <span class="math inline">\(\theta\)</span> (51). La forma de la
distribución inicial es triangular en ambos casos y el valor de mayor
probabilidad inicial es <span class="math inline">\(\theta = 0.50\)</span> y reflejamos que creemos que es menos
factible que el valor se encuentre en los extremos.</p>
<p>Podemos calcular el factor de Bayes para distintos datos observados:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Modelo 1, 3 posibles valores</span>
p_M1 &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">theta =</span> <span class="kw">c</span>(<span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>), <span class="dt">prior =</span> <span class="kw">c</span>(<span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.25</span>), 
  <span class="dt">modelo =</span><span class="st">&quot;M1&quot;</span>)

<span class="co"># Modelo 2, Creamos una inicial que puede tomar más valores</span>
p &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">24</span>, <span class="dv">1</span>)
p2 &lt;-<span class="st"> </span><span class="kw">c</span>(p, <span class="dv">24</span>, <span class="kw">sort</span>(p, <span class="dt">decreasing =</span> <span class="ot">TRUE</span>))
p_M2 &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">theta =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.02</span>), <span class="dt">prior =</span> p2 <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(p2), 
  <span class="dt">modelo =</span> <span class="st">&quot;M2&quot;</span>)

N &lt;-<span class="st"> </span><span class="dv">20</span> <span class="co"># estudiantes</span>
z &lt;-<span class="st"> </span><span class="dv">12</span> <span class="co"># éxitos</span>

dists_h &lt;-<span class="st"> </span><span class="kw">bind_rows</span>(p_M1, p_M2) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># base de datos horizontal</span>
<span class="st">    </span><span class="kw">group_by</span>(modelo) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(
        <span class="dt">Like =</span> theta <span class="op">^</span><span class="st"> </span>z <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>theta) <span class="op">^</span><span class="st"> </span>(N <span class="op">-</span><span class="st"> </span>z), <span class="co"># verosimilitud </span>
        <span class="dt">posterior =</span> (Like <span class="op">*</span><span class="st"> </span>prior) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(Like <span class="op">*</span><span class="st"> </span>prior)
      ) 

dists &lt;-<span class="st"> </span>dists_h <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># base de datos larga</span>
<span class="st">    </span><span class="kw">gather</span>(dist, valor, prior, Like, posterior) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">dist =</span> <span class="kw">factor</span>(dist, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;prior&quot;</span>, <span class="st">&quot;Like&quot;</span>, <span class="st">&quot;posterior&quot;</span>)))

factorBayes &lt;-<span class="st"> </span><span class="cf">function</span>(N, z){
  evidencia &lt;-<span class="st"> </span><span class="kw">bind_rows</span>(p_M1, p_M2) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># base de datos horizontal</span>
<span class="st">    </span><span class="kw">group_by</span>(modelo) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(
      <span class="dt">Like =</span> theta <span class="op">^</span><span class="st"> </span>z <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>theta) <span class="op">^</span><span class="st"> </span>(N <span class="op">-</span><span class="st"> </span>z), <span class="co"># verosimilitud </span>
      <span class="dt">posterior =</span> (Like <span class="op">*</span><span class="st"> </span>prior) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(Like <span class="op">*</span><span class="st"> </span>prior)
    ) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">summarise</span>(<span class="dt">evidencia =</span> <span class="kw">sum</span>(prior <span class="op">*</span><span class="st"> </span>Like))
  <span class="kw">print</span>(evidencia)
  <span class="kw">return</span>(evidencia[<span class="dv">1</span>, <span class="dv">2</span>] <span class="op">/</span><span class="st"> </span>evidencia[<span class="dv">2</span>, <span class="dv">2</span>])
}

<span class="kw">factorBayes</span>(<span class="dv">50</span>, <span class="dv">25</span>)
<span class="co">#&gt; # A tibble: 2 x 2</span>
<span class="co">#&gt;   modelo evidencia</span>
<span class="co">#&gt;   &lt;chr&gt;      &lt;dbl&gt;</span>
<span class="co">#&gt; 1 M1      4.44e-16</span>
<span class="co">#&gt; 2 M2      2.75e-16</span>
<span class="co">#&gt;   evidencia</span>
<span class="co">#&gt; 1      1.61</span>
<span class="kw">factorBayes</span>(<span class="dv">100</span>, <span class="dv">75</span>)
<span class="co">#&gt; # A tibble: 2 x 2</span>
<span class="co">#&gt;   modelo evidencia</span>
<span class="co">#&gt;   &lt;chr&gt;      &lt;dbl&gt;</span>
<span class="co">#&gt; 1 M1      9.46e-26</span>
<span class="co">#&gt; 2 M2      4.17e-26</span>
<span class="co">#&gt;   evidencia</span>
<span class="co">#&gt; 1      2.27</span>
<span class="kw">factorBayes</span>(<span class="dv">100</span>, <span class="dv">10</span>)
<span class="co">#&gt; # A tibble: 2 x 2</span>
<span class="co">#&gt;   modelo evidencia</span>
<span class="co">#&gt;   &lt;chr&gt;      &lt;dbl&gt;</span>
<span class="co">#&gt; 1 M1      1.36e-18</span>
<span class="co">#&gt; 2 M2      2.47e-16</span>
<span class="co">#&gt;   evidencia</span>
<span class="co">#&gt; 1   0.00549</span>
<span class="kw">factorBayes</span>(<span class="dv">40</span>, <span class="dv">38</span>)
<span class="co">#&gt; # A tibble: 2 x 2</span>
<span class="co">#&gt;   modelo   evidencia</span>
<span class="co">#&gt;   &lt;chr&gt;        &lt;dbl&gt;</span>
<span class="co">#&gt; 1 M1     0.000000279</span>
<span class="co">#&gt; 2 M2     0.00000895</span>
<span class="co">#&gt;   evidencia</span>
<span class="co">#&gt; 1    0.0312</span></code></pre>
<p>¿Cómo explicarías los resultados anteriores?</p>
<!-- $\theta$ 
puede tomar más valores, entonces si en una sucesión de volados observamos 
10% de águilas, el modelo simple no cuenta con un valor de $\theta$ cercano al 
resultado observado, pero el modelo complejo si. Por otra parte, para valores
de $\theta$ que se encuentran en ambos modelos la probabilidad inicial de esos 
valores es mayor en el caso del modelo simple. Por lo tanto si los datos 
observados resultan en valores de $\theta$ congruentes con el modelo simple, 
creeríamos en el modelo simple más que en el modelo más complicado.
-->
<p>La evidencia de un modelo <span class="math inline">\(p(x|M)\)</span> no dice mucho por si misma, si no que es mas
relevante en el contexto del <strong>factor de Bayes</strong> (la evidencia relativa de dos
modelos). Es importante recordar que la comparación de modelos nos habla
únicamente de la evidencia relativa de un modelo; sin embargo, puede que
ninguno de los modelos que estamos considerando sean adecuados para nuestros
datos, por lo que más adelante estudiaremos maneras de evaluar un modelo.</p>
</div>
<div id="calculo-de-la-distribucion-posterior" class="section level3 unnumbered">
<h3>Cálculo de la distribución posterior</h3>
<p>En la inferencia Bayesiana se requiere calcular el denominador de la fórmula
de Bayes <span class="math inline">\(p(x)\)</span>, es común que esto requiera que se calcule una integral
complicada; sin embargo, hay algunas maneras de evitar esto,</p>
<ol style="list-style-type: decimal">
<li><p>El camino tradicional consiste en usar funciones de verosimilitud con
dsitribuciones iniciales conjugadas. Cuando una distribución inicial es
conjugada de la verosimilitud resulta en una distribución posterior con la
misma forma funcional que la distribución inicial.</p></li>
<li><p>Otra alternativa es aproximar la integral numericamente. Cuando el espacio de
parámetros es de dimensión chica, se puede cubrir con una cuadrícula de
puntos y la integral se puede calcular sumando a través de dicha cuadrícula.
Sin embargo cuando el espacio de parámetros aumenta de dimensión el número de
puntos necesarios para la aproximación crece demasiado y hay que recurrir a otas
técnicas.</p></li>
<li><p>Se ha desarrollado una clase de métodos de simulación para poder calcular
la distribución posterior, estos se conocen como cadenas de Markov via Monte
Carlo (MCMC por sus siglas en inglés). El desarrollo de los métodos MCMC es lo
que ha propiciado el desarrollo de la estadística bayesiana en años recientes.</p></li>
</ol>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="probabilidad-subjetiva.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="distribuciones-conjugadas.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/tereom/est-computacional-2018/edit/master/09-analisis_bayesiano.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["est-computacional-2018.pdf", "est-computacional-2018.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
